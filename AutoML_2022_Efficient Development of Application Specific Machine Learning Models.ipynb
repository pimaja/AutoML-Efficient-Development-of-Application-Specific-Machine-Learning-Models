{"cells":[{"cell_type":"markdown","id":"eac8e618","metadata":{"id":"eac8e618"},"source":["## Important notes: \n"," - it is necessary to use a GPU for computing peak memory consumption, since we are using CUDA library for this.\n"," - it is necessary to make a free account on SigOpt platform to recieve own API token to be able to run experiments."]},{"cell_type":"markdown","id":"ba3a96df","metadata":{"id":"ba3a96df"},"source":["### Install neccessary packages"]},{"cell_type":"code","execution_count":null,"id":"386f34a7","metadata":{"scrolled":true,"id":"386f34a7"},"outputs":[],"source":["!pip install torch torchvision\n","!pip install sigopt\n","!pip install ipywidgets\n","!pip install plotly"]},{"cell_type":"markdown","id":"7b402096","metadata":{"id":"7b402096"},"source":["### Import packages"]},{"cell_type":"code","execution_count":null,"id":"b2d2c95e","metadata":{"id":"b2d2c95e"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas\n","import plotly.graph_objects as go\n","\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import sigopt\n","import os\n","import time "]},{"cell_type":"markdown","id":"319f50a6","metadata":{"id":"319f50a6"},"source":["### Set up connection to SigOpt platform and using GPU"]},{"cell_type":"code","execution_count":null,"id":"9f02f8b8","metadata":{"id":"9f02f8b8"},"outputs":[],"source":["os.environ[\"SIGOPT_API_TOKEN\"] = #SIGOPT API TOKEN\n","os.environ[\"SIGOPT_PROJECT\"] = \"model_optimization_00000\"\n","%reload_ext sigopt\n","\n","dtype = torch.float\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n","print(dtype, device)"]},{"cell_type":"markdown","id":"adf85b0e","metadata":{"id":"adf85b0e"},"source":["### Model "]},{"cell_type":"code","execution_count":null,"id":"81e9d9d1","metadata":{"id":"81e9d9d1"},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self, image_width, image_channels, num_classes, params):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(image_channels, params[\"conv1_out_channels\"], params[\"conv1_kernel_size\"], padding=\"same\")\n","        self.conv2 = nn.Conv2d(params[\"conv1_out_channels\"], params[\"conv2_out_channels\"], params[\"conv2_kernel_size\"], padding=\"same\")\n","        self.conv3 = nn.Conv2d(params[\"conv2_out_channels\"], params[\"conv3_out_channels\"], params[\"conv3_kernel_size\"], padding=\"same\")\n","        self.pool = nn.MaxPool2d(params[\"pool_kernel_size\"])\n","        self.batchnorm1 = nn.BatchNorm2d(params[\"conv1_out_channels\"])\n","        self.batchnorm2 = nn.BatchNorm2d(params[\"conv2_out_channels\"])\n","        self.batchnorm3 = nn.BatchNorm2d(params[\"conv3_out_channels\"])\n","        dim_after_two_pools = int(image_width / (params[\"pool_kernel_size\"] ** 2))\n","        self.fc1 = nn.Linear(params[\"conv3_out_channels\"] * dim_after_two_pools ** 2, params[\"fc1_out_features\"])\n","        self.fc2 = nn.Linear(params[\"fc1_out_features\"], params[\"fc2_out_features\"])\n","        self.fc3 = nn.Linear(params[\"fc2_out_features\"], num_classes)\n","        self.dropout = nn.Dropout(params[\"dropout_probability\"])\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = self.batchnorm1(x)\n","        x = F.relu(self.conv2(x))\n","        x = self.pool(self.batchnorm2(x))\n","        x = F.relu(self.conv3(x))\n","        x = self.pool(self.batchnorm3(x))\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x"]},{"cell_type":"markdown","id":"a1d063a4","metadata":{"id":"a1d063a4"},"source":["### Loss and optimizer "]},{"cell_type":"code","execution_count":null,"id":"732b59a4","metadata":{"id":"732b59a4"},"outputs":[],"source":["def loss_and_optimizer(model, learning_rate):\n","    loss = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    return loss, optimizer"]},{"cell_type":"markdown","id":"7fc93b43","metadata":{"id":"7fc93b43"},"source":["### Train function"]},{"cell_type":"code","execution_count":null,"id":"d03a0205","metadata":{"id":"d03a0205"},"outputs":[],"source":["def train(dataloader, model, criterion, optimizer, epochs):\n","    loss_array = []\n","    accuracy_array = []\n","    \n","    for t in range(epochs):\n","        print(f\"\\nEpoch {t+1}\\n-------------------------------\")\n","        size = len(dataloader.dataset)\n","        num_batches = len(dataloader)\n","        train_loss, correct = 0, 0\n","        \n","        model.train()     \n","        for batch, (X, y) in enumerate(dataloader):\n","            X = X.to(dtype=dtype, device=device)\n","            y = y.to(device=device)\n","            \n","            # Compute prediction error\n","            pred = model(X)\n","            loss = criterion(pred, y)\n","            \n","            # Update \n","            train_loss += loss.item()\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","            # Backpropagation\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            if batch % 1000 == 0:\n","                loss, current = loss.item(), batch * len(X)\n","                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","                \n","        train_loss /= num_batches\n","        loss_array.append(train_loss)       \n","        correct /= size\n","        accuracy_array.append(100*correct)\n","        print(f\"Train Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")"]},{"cell_type":"markdown","id":"75bc8bf1","metadata":{"id":"75bc8bf1"},"source":["### Test function"]},{"cell_type":"code","execution_count":null,"id":"f42c491f","metadata":{"id":"f42c491f"},"outputs":[],"source":["def test(dataloader, model, criterion):\n","    # evaluate mode\n","    model.eval()\n","    \n","    \"\"\"Compute the different metrics\"\"\"\n","    \n","    metrics = {}\n","    \n","    # accuracy and loss\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    loss, correct = 0, 0\n","    \n","    # inference time\n","    total_time = 0\n","    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n","    \n","    # gpu warm-up\n","    dummy_input = torch.randn(8, 3, 32, 32, dtype=dtype).to(device) # for CIFAR 10 -> batch_size=8, image_channels=3, image_width=32\n","    for _ in range(10):\n","        _ = model(dummy_input)\n","        \n","    # reset peak memory stats - just compute the peak memory for inference\n","    torch.cuda.reset_peak_memory_stats()\n","    \n","    # inference\n","    with torch.no_grad():\n","        for X, y in dataloader:        \n","            X = X.to(dtype=dtype, device=device)\n","            y = y.to(device=device)  \n","            \n","            # record time\n","            starter.record()\n","            pred = model(X) \n","            ender.record()\n","      \n","            # wait for gpu sync\n","            torch.cuda.synchronize()\n","            \n","            curr_time = starter.elapsed_time(ender)\n","            total_time += curr_time  \n","            loss += criterion(pred, y).item()\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()            \n","   \n","    #peak memory - in mB per image \n","    stats = torch.cuda.memory_stats(device=device)\n","    peak_memory = stats[\"allocated_bytes.all.peak\"] / (2 ** 20)\n","    peak_memory /= size\n","    \n","    #storage memory - in mB\n","    torch.save(model.state_dict(), \"tmp2.pt\")\n","    size_mb = os.path.getsize(\"tmp2.pt\")/1e6\n","    os.remove('tmp2.pt')\n","    storage_memory = np.round(size_mb, 2)\n","\n","    # accuracy and loss\n","    correct /= size  \n","    \n","    # inference time - in seconds per image\n","    inference_time = total_time / size\n","    \n","    metrics['accuracy'] = correct\n","    metrics['inference_time'] = inference_time\n","    metrics['peak_memory'] = peak_memory\n","    metrics['storage_memory'] = storage_memory\n","    \n","    print(f\"Test accuracy: {(100*correct):>0.1f}% \\n\")\n","    \n","    return metrics"]},{"cell_type":"markdown","id":"19bef800","metadata":{"id":"19bef800"},"source":["### Radar plot function"]},{"cell_type":"code","execution_count":null,"id":"5c5e1898","metadata":{"id":"5c5e1898"},"outputs":[],"source":["class ComplexRadar():\n","    def __init__(self, fig, variables, ranges, n_ordinate_levels=6):\n","        angles = np.arange(0, 360, 360./len(variables))\n","\n","        axes = [fig.add_axes([0.1,0.1,0.9,0.9], polar=True,\n","            label = \"axes{}\".format(i)) \n","            for i in range(len(variables))]\n","\n","        l, text = axes[0].set_thetagrids(angles, labels=variables)\n","\n","        [[txt.set_fontweight('bold'),\n","              txt.set_fontsize(12),\n","              txt.set_position((0,-0.2))] for txt in text]\n","\n","        for ax in axes[1:]:\n","            ax.patch.set_visible(False)\n","            ax.grid(\"off\")\n","            ax.xaxis.set_visible(False)\n","\n","        for i, ax in enumerate(axes):\n","            grid = np.linspace(*ranges[i], num=n_ordinate_levels)\n","            gridlabel = [\"{}\".format(round(x,2)) for x in grid]\n","\n","            gridlabel[0] = \"\" # clean up origin\n","            ax.set_rgrids(grid, labels=gridlabel,angle=angles[i])\n","\n","            ax.set_ylim(*ranges[i])\n","        \n","        # variables for plotting\n","        self.angle = np.deg2rad(np.r_[angles, angles[0]])\n","        self.ranges = ranges\n","        self.ax = axes[0]\n","\n","    def plot(self, data, *args, **kw):\n","        sdata = self.scale_data(data, self.ranges)\n","        self.ax.plot(self.angle, np.r_[sdata, sdata[0]], *args, **kw)\n","\n","    def fill(self, data, *args, **kw):\n","        sdata = self.scale_data(data, self.ranges)\n","        self.ax.fill(self.angle, np.r_[sdata, sdata[0]], *args, **kw)\n","\n","    def scale_data(self, data, ranges):\n","        \"\"\"scales data[1:] to ranges[0]\"\"\"\n","        for d, (y1, y2) in zip(data[1:], ranges[1:]):\n","            assert (y1 <= d <= y2) or (y2 <= d <= y1)\n","        x1, x2 = ranges[0]\n","        d = data[0]\n","        sdata = [d]\n","        for d, (y1, y2) in zip(data[1:], ranges[1:]):\n","            if y1 > y2:\n","                d = _invert(d, (y1, y2))\n","                y1, y2 = y2, y1\n","            sdata.append((d-y1) / (y2-y1) * (x2 - x1) + x1)\n","        return sdata"]},{"cell_type":"markdown","id":"9c6b5ec3","metadata":{"id":"9c6b5ec3"},"source":["### Optimize with SigOpt"]},{"cell_type":"code","execution_count":null,"id":"52aaf08a","metadata":{"id":"52aaf08a"},"outputs":[],"source":["def train_and_track_model(trainset, testset, testbatchsize, image_width, image_channels, classes, run):\n","\n","    \"\"\"\n","    Creates a function which trains a model while tracking artifacts\n","    \"\"\"\n","  \n","    run.log_model('CNN with 3 convolutional, 2 pools and 2 fully connected layers')\n","    run.log_dataset(name='CIFAR_10')\n","\n","    run.params.setdefault(\"batchsize\", 32)\n","    run.params.setdefault(\"learning_rate\", 0.001)\n","    run.params.setdefault(\"epochs\", 5)\n","    run.params.setdefault(\"conv1_out_channels\", 32)\n","    run.params.setdefault(\"conv1_kernel_size\", 5)\n","    run.params.setdefault(\"pool_kernel_size\", 2)\n","    run.params.setdefault(\"conv2_out_channels\", 64)\n","    run.params.setdefault(\"conv2_kernel_size\", 5)\n","    run.params.setdefault(\"conv3_out_channels\", 128)\n","    run.params.setdefault(\"conv3_kernel_size\", 5)\n","    run.params.setdefault(\"fc1_out_features\", 512)\n","    run.params.setdefault(\"fc2_out_features\", 256)\n","    run.params.setdefault(\"dropout_probability\", 0.2)\n","\n","    # construct training data loader\n","    train_loader = torch.utils.data.DataLoader(trainset, batch_size=run.params.batchsize, shuffle=\"True\")\n","    \n","    # construct testing data loader\n","    test_loader = torch.utils.data.DataLoader(testset, batch_size=testbatchsize, shuffle=\"True\")\n","\n","    # initialize the model\n","    model = Model(image_width, image_channels, len(classes), run.params)\n","    \n","    # change dtype and device of the model\n","    model.to(dtype=dtype, device=device)\n","\n","    # define loss and optimizer\n","    loss, optimizer = loss_and_optimizer(model, run.params.learning_rate)\n","    \n","    # train network\n","    train(train_loader, model, loss, optimizer, run.params.epochs)\n","\n","    # test network, returns values of metrics\n","    metrics = test(test_loader, model, loss)\n","    \n","    # log metrics\n","    for name, value in metrics.items():\n","        run.log_metric(name, value)"]},{"cell_type":"markdown","id":"90121a9a","metadata":{"id":"90121a9a"},"source":["### Download CIFAR10 datasets"]},{"cell_type":"code","execution_count":null,"id":"cdd6e864","metadata":{"id":"cdd6e864"},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4, padding_mode='reflect'), \n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","\n","classes = testset.classes\n","channels, width, height = trainset[0][0].shape\n","\n","print(channels, width, height, classes)"]},{"cell_type":"markdown","id":"d1bbea5c","metadata":{"id":"d1bbea5c"},"source":["## Experiment 1: Optimizing accuracy while only keeping track of the other metrics - with test batch size 10"]},{"cell_type":"code","execution_count":null,"id":"1b3015f9","metadata":{"id":"1b3015f9"},"outputs":[],"source":["experiment_acc10 = sigopt.create_experiment(\n","  name=\"AutoML_acc_test_batch_size_10\",\n","  type=\"offline\",\n","  parameters=[\n","      dict(name=\"batchsize\", type=\"int\", bounds=dict(min=8, max=64)),\n","      dict(name=\"learning_rate\", type=\"double\", bounds=dict(min=1.0e-5, max=0.5), transformation=\"log\"),\n","      dict(name=\"epochs\", type=\"int\", bounds=dict(min=3, max=20)),\n","      dict(name=\"conv1_out_channels\", type=\"int\", bounds=dict(min=16, max=64)),\n","      dict(name=\"conv1_kernel_size\", type=\"int\", grid=[3,5,7,9]),\n","      dict(name=\"pool_kernel_size\", type=\"int\", bounds=dict(min=2, max=3)),\n","      dict(name=\"conv2_out_channels\", type=\"int\", bounds=dict(min=32, max=128)),\n","      dict(name=\"conv2_kernel_size\", type=\"int\", grid=[3,5,7,9]),\n","      dict(name=\"conv3_out_channels\", type=\"int\", bounds=dict(min=64, max=256)),\n","      dict(name=\"conv3_kernel_size\", type=\"int\", grid=[3,5,7,9]),\n","      dict(name=\"fc1_out_features\", type=\"int\", bounds=dict(min=256, max=1024)),\n","      dict(name=\"fc2_out_features\", type=\"int\", bounds=dict(min=64, max=512)),\n","      dict(name=\"dropout_probability\", type=\"double\", bounds=dict(min=0.01, max=0.5))\n","  ],\n","  metrics=[\n","      dict(name=\"accuracy\", objective=\"maximize\", strategy=\"optimize\"),\n","      dict(name=\"inference_time\", objective=\"minimize\", strategy=\"store\"),\n","      dict(name=\"peak_memory\", objective=\"minimize\", strategy=\"store\"),\n","      dict(name=\"storage_memory\", objective=\"minimize\", strategy=\"store\")\n","  ],\n","  parallel_bandwidth=1,\n","  budget=100,\n",")"]},{"cell_type":"code","execution_count":null,"id":"d1f29842","metadata":{"id":"d1f29842"},"outputs":[],"source":["for run in experiment_acc10.loop():\n","    with run:\n","        train_and_track_model(trainset, testset, 10, width, channels, classes, run)"]},{"cell_type":"code","execution_count":null,"id":"a17b56b9","metadata":{"id":"a17b56b9"},"outputs":[],"source":["# get the best run for the experiment\n","best_runs_acc10 = experiment_acc10.get_best_runs()\n","best_runs_acc10 = list(best_runs_acc10)\n","for run in best_runs_acc10:\n","    print(run)"]},{"cell_type":"code","execution_count":null,"id":"be6b1215","metadata":{"id":"be6b1215"},"outputs":[],"source":["# plot results\n","runs = experiment_acc10.get_runs()\n","runs = list(runs)\n","runs = runs[::-1]\n","length = len(runs)\n","\n","accuracies = []\n","times = []\n","pm = []\n","sm = []\n","\n","for run in runs:\n","    if(run.state == \"completed\"):\n","        accuracies.append(run.values[\"accuracy\"].value)\n","        times.append(run.values[\"inference_time\"].value)\n","        pm.append(run.values[\"peak_memory\"].value)\n","        sm.append(run.values[\"storage_memory\"].value)\n","        \n","fig, axs = plt.subplots(2, 2, figsize=(16,12), sharex=True)\n","axs[0, 0].plot(accuracies, '-o')\n","axs[0, 0].set_title('Accuracy')\n","axs[0, 0].set(ylabel='[%]')\n","axs[0, 1].plot(times, '-o')\n","axs[0, 1].set_title('Inference time')\n","axs[0, 1].set(ylabel='[seconds per image]')\n","axs[1, 0].plot(pm, '-o')\n","axs[1, 0].set_title('Peak memory consumption')\n","axs[1, 0].set(xlabel='Runs', ylabel='[MB per image]')\n","axs[1, 1].plot(sm, '-o')\n","axs[1, 1].set_title('Storage memory')\n","axs[1, 1].set(xlabel='Runs', ylabel='[MB]')\n","fig.tight_layout()"]},{"cell_type":"markdown","id":"a3980735","metadata":{"id":"a3980735"},"source":["## Experiment 1: Optimizing accuracy while only keeping track of the other metrics - with test batch size 20"]},{"cell_type":"code","execution_count":null,"id":"d31e9fc1","metadata":{"id":"d31e9fc1"},"outputs":[],"source":["experiment_acc20 = sigopt.create_experiment(\n","  name=\"AutoML_acc_test_batch_size_20\",\n","  type=\"offline\",\n","  parameters=[\n","      dict(name=\"batchsize\", type=\"int\", bounds=dict(min=8, max=64)),\n","      dict(name=\"learning_rate\", type=\"double\", bounds=dict(min=1.0e-5, max=0.5), transformation=\"log\"),\n","      dict(name=\"epochs\", type=\"int\", bounds=dict(min=3, max=20)),\n","      dict(name=\"conv1_out_channels\", type=\"int\", bounds=dict(min=16, max=64)),\n","      dict(name=\"conv1_kernel_size\", type=\"int\", grid=[3,5,7,9]),\n","      dict(name=\"pool_kernel_size\", type=\"int\", bounds=dict(min=2, max=3)),\n","      dict(name=\"conv2_out_channels\", type=\"int\", bounds=dict(min=32, max=128)),\n","      dict(name=\"conv2_kernel_size\", type=\"int\", grid=[3,5,7,9]),\n","      dict(name=\"conv3_out_channels\", type=\"int\", bounds=dict(min=64, max=256)),\n","      dict(name=\"conv3_kernel_size\", type=\"int\", grid=[3,5,7,9]),\n","      dict(name=\"fc1_out_features\", type=\"int\", bounds=dict(min=256, max=1024)),\n","      dict(name=\"fc2_out_features\", type=\"int\", bounds=dict(min=64, max=512)),\n","      dict(name=\"dropout_probability\", type=\"double\", bounds=dict(min=0.01, max=0.5))\n","  ],\n","  metrics=[\n","      dict(name=\"accuracy\", objective=\"maximize\", strategy=\"optimize\"),\n","      dict(name=\"inference_time\", objective=\"minimize\", strategy=\"store\"),\n","      dict(name=\"peak_memory\", objective=\"minimize\", strategy=\"store\"),\n","      dict(name=\"storage_memory\", objective=\"minimize\", strategy=\"store\")\n","  ],\n","  parallel_bandwidth=1,\n","  budget=100,\n",")"]},{"cell_type":"code","execution_count":null,"id":"ce655015","metadata":{"id":"ce655015"},"outputs":[],"source":["for run in experiment_acc20.loop():\n","    with run:\n","        train_and_track_model(trainset, testset, 20, width, channels, classes, run)"]},{"cell_type":"code","execution_count":null,"id":"6dc0ebf6","metadata":{"id":"6dc0ebf6"},"outputs":[],"source":["# get the best run for the experiment\n","best_runs_acc20 = experiment_acc20.get_best_runs()\n","best_runs_acc20 = list(best_runs_acc20)\n","for run in best_runs_acc20:\n","    print(run)"]},{"cell_type":"code","execution_count":null,"id":"4612af17","metadata":{"id":"4612af17"},"outputs":[],"source":["# plot results\n","runs = experiment_acc20.get_runs()\n","runs = list(runs)\n","runs = runs[::-1]\n","length = len(runs)\n","\n","accuracies = []\n","times = []\n","pm = []\n","sm = []\n","\n","for run in runs:\n","    if(run.state == \"completed\"):\n","        accuracies.append(run.values[\"accuracy\"].value)\n","        times.append(run.values[\"inference_time\"].value)\n","        pm.append(run.values[\"peak_memory\"].value)\n","        sm.append(run.values[\"storage_memory\"].value)\n","        \n","fig, axs = plt.subplots(2, 2, figsize=(16,12), sharex=True)\n","axs[0, 0].plot(accuracies, '-o')\n","axs[0, 0].set_title('Accuracy')\n","axs[0, 0].set(ylabel='[%]')\n","axs[0, 1].plot(times, '-o')\n","axs[0, 1].set_title('Inference time')\n","axs[0, 1].set(ylabel='[seconds per image]')\n","axs[1, 0].plot(pm, '-o')\n","axs[1, 0].set_title('Peak memory consumption')\n","axs[1, 0].set(xlabel='Runs', ylabel='[MB per image]')\n","axs[1, 1].plot(sm, '-o')\n","axs[1, 1].set_title('Storage memory')\n","axs[1, 1].set(xlabel='Runs', ylabel='[MB]')\n","fig.tight_layout()"]},{"cell_type":"markdown","id":"372a9555","metadata":{"id":"372a9555"},"source":["## Experiment 2: Constraint Active Search - with test batch size 10"]},{"cell_type":"code","execution_count":null,"id":"cfb4444b","metadata":{"id":"cfb4444b"},"outputs":[],"source":["experiment_cas10 = sigopt.create_experiment(\n","  name=\"AutoML_cas_test_batch_size_10\",\n","  type=\"offline\",\n","  parameters=[\n","      dict(name=\"batchsize\", type=\"int\", bounds=dict(min=8, max=64)),\n","      dict(name=\"learning_rate\", type=\"double\", bounds=dict(min=1.0e-5, max=0.5), transformation=\"log\"),\n","      dict(name=\"epochs\", type=\"int\", bounds=dict(min=3, max=20)),\n","      dict(name=\"conv1_out_channels\", type=\"int\", bounds=dict(min=16, max=64)),\n","      dict(name=\"conv1_kernel_size\", type=\"int\", grid=[3,5,7,9]),\n","      dict(name=\"pool_kernel_size\", type=\"int\", bounds=dict(min=2, max=3)),\n","      dict(name=\"conv2_out_channels\", type=\"int\", bounds=dict(min=32, max=128)),\n","      dict(name=\"conv2_kernel_size\", type=\"int\", grid=[3,5,7,9]),\n","      dict(name=\"conv3_out_channels\", type=\"int\", bounds=dict(min=64, max=256)),\n","      dict(name=\"conv3_kernel_size\", type=\"int\", grid=[3,5,7,9]),\n","      dict(name=\"fc1_out_features\", type=\"int\", bounds=dict(min=256, max=1024)),\n","      dict(name=\"fc2_out_features\", type=\"int\", bounds=dict(min=64, max=512)),\n","      dict(name=\"dropout_probability\", type=\"double\", bounds=dict(min=0.01, max=0.5))\n","  ],\n","  metrics=[\n","      dict(name=\"accuracy\", objective=\"maximize\", strategy=\"constraint\", threshold=0.75),\n","      dict(name=\"inference_time\", objective=\"minimize\", strategy=\"constraint\", threshold=0.3),\n","      dict(name=\"peak_memory\", objective=\"minimize\", strategy=\"constraint\", threshold=0.01),\n","      dict(name=\"storage_memory\", objective=\"minimize\", strategy=\"constraint\", threshold=10)\n","  ],\n","  parallel_bandwidth=1,\n","  budget=100,\n",")"]},{"cell_type":"code","execution_count":null,"id":"3ee9a636","metadata":{"id":"3ee9a636"},"outputs":[],"source":["for run in experiment_cas10.loop():\n","    with run:\n","        train_and_track_model(trainset, testset, 10, width, channels, classes, run)"]},{"cell_type":"code","execution_count":null,"id":"fcc6bf87","metadata":{"id":"fcc6bf87"},"outputs":[],"source":["# get the best run for the experiment\n","best_runs_cas10 = experiment_cas10.get_best_runs()\n","best_runs_cas10 = list(best_runs_cas10)\n","for run in best_runs_cas10:\n","    print(run)"]},{"cell_type":"code","execution_count":null,"id":"21ecdee4","metadata":{"id":"21ecdee4"},"outputs":[],"source":["# how many best runs - runs that satisfy constraints - is there\n","print(len(best_runs_cas10))"]},{"cell_type":"code","execution_count":null,"id":"f6d10d9a","metadata":{"id":"f6d10d9a"},"outputs":[],"source":["# plot results\n","runs = experiment_cas10.get_runs()\n","runs = list(runs)\n","runs = runs[::-1]\n","length = len(runs)\n","\n","accuracies = []\n","times = []\n","pm = []\n","sm = []\n","\n","for run in runs:\n","    if(run.state == \"completed\"):\n","        accuracies.append(run.values[\"accuracy\"].value)\n","        times.append(run.values[\"inference_time\"].value)\n","        pm.append(run.values[\"peak_memory\"].value)\n","        sm.append(run.values[\"storage_memory\"].value)\n","        \n","fig, axs = plt.subplots(2, 2, figsize=(16,12), sharex=True)\n","axs[0, 0].plot(accuracies, '-o')\n","axs[0, 0].set_title('Accuracy')\n","axs[0, 0].set(ylabel='[%]')\n","axs[0, 1].plot(times, '-o')\n","axs[0, 1].set_title('Inference time')\n","axs[0, 1].set(ylabel='[seconds per image]')\n","axs[1, 0].plot(pm, '-o')\n","axs[1, 0].set_title('Peak memory consumption')\n","axs[1, 0].set(xlabel='Runs', ylabel='[MB per image]')\n","axs[1, 1].plot(sm, '-o')\n","axs[1, 1].set_title('Storage memory')\n","axs[1, 1].set(xlabel='Runs', ylabel='[MB]')\n","fig.tight_layout()"]},{"cell_type":"markdown","id":"ccb65ec4","metadata":{"id":"ccb65ec4"},"source":["## Experiment 2: Constraint Active Search - with test batch size 20"]},{"cell_type":"code","execution_count":null,"id":"9930e7e8","metadata":{"id":"9930e7e8"},"outputs":[],"source":["experiment_cas20 = sigopt.create_experiment(\n","  name=\"AutoML_cas_test_batch_size_20\",\n","  type=\"offline\",\n","  parameters=[\n","      dict(name=\"batchsize\", type=\"int\", bounds=dict(min=8, max=64)),\n","      dict(name=\"learning_rate\", type=\"double\", bounds=dict(min=1.0e-5, max=0.5), transformation=\"log\"),\n","      dict(name=\"epochs\", type=\"int\", bounds=dict(min=3, max=20)),\n","      dict(name=\"conv1_out_channels\", type=\"int\", bounds=dict(min=16, max=64)),\n","      dict(name=\"conv1_kernel_size\", type=\"int\", grid=[3,5,7,9]),\n","      dict(name=\"pool_kernel_size\", type=\"int\", bounds=dict(min=2, max=3)),\n","      dict(name=\"conv2_out_channels\", type=\"int\", bounds=dict(min=32, max=128)),\n","      dict(name=\"conv2_kernel_size\", type=\"int\", grid=[3,5,7,9]),\n","      dict(name=\"conv3_out_channels\", type=\"int\", bounds=dict(min=64, max=256)),\n","      dict(name=\"conv3_kernel_size\", type=\"int\", grid=[3,5,7,9]),\n","      dict(name=\"fc1_out_features\", type=\"int\", bounds=dict(min=256, max=1024)),\n","      dict(name=\"fc2_out_features\", type=\"int\", bounds=dict(min=64, max=512)),\n","      dict(name=\"dropout_probability\", type=\"double\", bounds=dict(min=0.01, max=0.5))\n","  ],\n","  metrics=[\n","      dict(name=\"accuracy\", objective=\"maximize\", strategy=\"constraint\", threshold=0.75),\n","      dict(name=\"inference_time\", objective=\"minimize\", strategy=\"constraint\", threshold=0.3),\n","      dict(name=\"peak_memory\", objective=\"minimize\", strategy=\"constraint\", threshold=0.01),\n","      dict(name=\"storage_memory\", objective=\"minimize\", strategy=\"constraint\", threshold=10)\n","  ],\n","  parallel_bandwidth=1,\n","  budget=100,\n",")"]},{"cell_type":"code","execution_count":null,"id":"12dffbd5","metadata":{"id":"12dffbd5"},"outputs":[],"source":["for run in experiment_cas20.loop():\n","    with run:\n","        train_and_track_model(trainset, testset, 20, width, channels, classes, run)"]},{"cell_type":"code","execution_count":null,"id":"2f104b90","metadata":{"id":"2f104b90"},"outputs":[],"source":["# get the best run for the experiment\n","best_runs_cas20 = experiment_cas20.get_best_runs()\n","best_runs_cas20 = list(best_runs_cas20)\n","for run in best_runs_cas20:\n","    print(run)"]},{"cell_type":"code","execution_count":null,"id":"1ceec1e8","metadata":{"id":"1ceec1e8"},"outputs":[],"source":["# how many best runs - runs that satisfy constraints - is there\n","print(len(best_runs_cas20))"]},{"cell_type":"code","execution_count":null,"id":"f41a7c15","metadata":{"id":"f41a7c15"},"outputs":[],"source":["# plot results\n","runs = experiment_cas20.get_runs()\n","runs = list(runs)\n","runs = runs[::-1]\n","length = len(runs)\n","\n","accuracies = []\n","times = []\n","pm = []\n","sm = []\n","\n","for run in runs:\n","    if(run.state == \"completed\"):\n","        accuracies.append(run.values[\"accuracy\"].value)\n","        times.append(run.values[\"inference_time\"].value)\n","        pm.append(run.values[\"peak_memory\"].value)\n","        sm.append(run.values[\"storage_memory\"].value)\n","        \n","fig, axs = plt.subplots(2, 2, figsize=(16,12), sharex=True)\n","axs[0, 0].plot(accuracies, '-o')\n","axs[0, 0].set_title('Accuracy')\n","axs[0, 0].set(ylabel='[%]')\n","axs[0, 1].plot(times, '-o')\n","axs[0, 1].set_title('Inference time')\n","axs[0, 1].set(ylabel='[seconds per image]')\n","axs[1, 0].plot(pm, '-o')\n","axs[1, 0].set_title('Peak memory consumption')\n","axs[1, 0].set(xlabel='Runs', ylabel='[MB per image]')\n","axs[1, 1].plot(sm, '-o')\n","axs[1, 1].set_title('Storage memory')\n","axs[1, 1].set(xlabel='Runs', ylabel='[MB]')\n","fig.tight_layout()"]},{"cell_type":"markdown","id":"be49e1e9","metadata":{"id":"be49e1e9"},"source":["## Radar plot for comparison of best results from Experiment 1 and Experiment 2\n","\n","Note: It is necessary to find IDs of runs with the best results from current runs of experiments. The easiest way to get those is to access SigOpt platform -> current project -> experiment -> \"History\", and by using the sorting option find the run ID with highest accuracy for Experiment 1, and IDs of runs with highest accuracy, lowest inference time, lowest peak memory consumption, and lowest storage memory for Experiment 2. "]},{"cell_type":"code","execution_count":null,"id":"d427d09d","metadata":{"id":"d427d09d"},"outputs":[],"source":["# test batch size 10\n","id_baseline_run = #Run_ID\n","id_CAS_acc_run = #Run_ID\n","id_CAS_time_run = #Run_ID\n","id_CAS_pm_run = #Run_ID\n","id_CAS_sm_run = #Run_ID\n","\n","# test batch size 20\n","#id_baseline_run = #Run_ID\n","#id_CAS_acc_run = #Run_ID\n","#id_CAS_time_run = #Run_ID\n","#id_CAS_pm_run = #Run_ID\n","#id_CAS_sm_run = #Run_ID\n","\n","run_baseline = sigopt.get_run(id_baseline_run).values\n","run_CAS_acc = sigopt.get_run(id_CAS_acc_run).values\n","run_CAS_time = sigopt.get_run(id_CAS_time_run).values\n","run_CAS_pm = sigopt.get_run(id_CAS_pm_run).values\n","run_CAS_sm = sigopt.get_run(id_CAS_sm_run).values\n","\n","metrics_baseline = []\n","metrics_CAS_acc = []\n","metrics_CAS_time = []\n","metrics_CAS_pm = []\n","metrics_CAS_sm = []\n","\n","variables = ['accuracy', 'inference_time', 'peak_memory', 'storage_memory']\n","ranges = [(0, 1), (0, 0.4), (0, 0.06), (1, 60)] \n","\n","for k in variables:\n","    metrics_baseline.append(run_baseline[k].value)\n","    metrics_CAS_acc.append(run_CAS_acc[k].value)\n","    metrics_CAS_time.append(run_CAS_time[k].value)\n","    metrics_CAS_pm.append(run_CAS_pm[k].value)\n","    metrics_CAS_sm.append(run_CAS_sm[k].value)\n","\n","# plotting\n","fig1 = plt.figure(figsize=(6, 6))\n","fig1.suptitle(\"Test batch size 1\", x = 0.55, y=1.25, fontweight = \"bold\")\n","radar = ComplexRadar(fig1, variables, ranges)\n","\n","radar.plot(metrics_baseline, label=\"E1: Best accuracy\")\n","radar.fill(metrics_baseline, alpha=0.2)\n","\n","radar.plot(metrics_CAS_acc, label=\"E2: Best accuracy\")\n","radar.fill(metrics_CAS_acc, alpha=0.2)\n","\n","radar.plot(metrics_CAS_time, label=\"E2: Best inference time\")\n","radar.fill(metrics_CAS_time, alpha=0.2)\n","\n","radar.plot(metrics_CAS_pm, label=\"E2: Best peak memory\")\n","radar.fill(metrics_CAS_pm, alpha=0.2)\n","\n","radar.plot(metrics_CAS_sm, label=\"E2: Best storage memory\")\n","radar.fill(metrics_CAS_sm, alpha=0.2)\n","    \n","radar.ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.20),\n","      fancybox=True, ncol=5, fontsize=12)\n","plt.show()"]},{"cell_type":"markdown","source":["# Parallel Coordinate Plot\n","\n","Note: these are the constraints used for the experiment"],"metadata":{"id":"cAQBXlxwwZdH"},"id":"cAQBXlxwwZdH"},{"cell_type":"code","execution_count":null,"id":"09c0b2c0","metadata":{"id":"09c0b2c0"},"outputs":[],"source":["constraints = {}\n","constraints['accuracy'] = 0.75\n","constraints['inference_time'] = 0.3\n","constraints['peak_memory'] = 100.0/10000.0\n","constraints['storage_memory'] = 10.0"]},{"cell_type":"code","source":["experiment_bs_10 = \" insert experiment ID for batch size of 10 \" # it is not nessesary that this experiment have a btch size of 10, it can be any experiment as long as all of the parameters and metrics are there. E.g., other optimization algorithms\n","experiment_bs_10 = sigopt.get_experiment(experiment_bs_10)\n","\n","df_bs_10 = pd.DataFrame([])\n","\n","for run in experiment_bs_10.get_runs():\n","\n","  if df_bs_10.empty:\n","    \n","    metrics_names = [metric for metric in run.values.keys()]\n","    hyperparameters_names = [hyperparameter for hyperparameter in run.assignments.keys()]\n","    \n","    df_bs_10 = pd.DataFrame([], columns = metrics_names + hyperparameters_names)\n","  \n","  metrics = {}\n","  for m in run.values: metrics[run.values[m].name] = run.values[m].value\n","\n","  metrics.update(run.assignments)\n","\n","  df_bs_10 = df_bs_10.append(metrics, ignore_index=True)\n","\n","df_bs_10['peak_memory'] = df_bs_10['peak_memory'] / 10000.0\n","\n","hue_bs_10 = pd.DataFrame([(df_bs_10['accuracy'] > constraints['accuracy']),\n","                        (df_bs_10['inference_time'] < constraints['inference_time']),\n","                        (df_bs_10['peak_memory'] < constraints['peak_memory']),\n","                        (df_bs_10['storage_memory'] < constraints['storage_memory'])]).all()\n","\n","hue_bs_10.name = 'Constraints'\n","\n","hue_bs_10[hue_bs_10 == False] = 0\n","hue_bs_10[hue_bs_10 == True] = 1\n","\n","df_bs_10 = pd.concat([df_bs_10, hue_bs_10], axis = 1)"],"metadata":{"id":"3shjc7T-w92W"},"id":"3shjc7T-w92W","execution_count":null,"outputs":[]},{"cell_type":"code","source":["experiment_bs_20 = \" insert experiment ID for batch size of 20 \" # it is not nessesary that this experiment have a btch size of 10, it can be any experiment as long as all of the parameters and metrics are there. E.g., other optimization algorithms\n","experiment_bs_20 = sigopt.get_experiment(experiment_bs_20)\n","\n","df_bs_20 = pd.DataFrame([])\n","\n","for run in experiment_bs_20.get_runs():\n","\n","  if df_bs_20.empty:\n","    \n","    metrics_names = [metric for metric in run.values.keys()]\n","    hyperparameters_names = [hyperparameter for hyperparameter in run.assignments.keys()]\n","    \n","    df_bs_20 = pd.DataFrame([], columns = metrics_names + hyperparameters_names)\n","  \n","  metrics = {}\n","  for m in run.values: metrics[run.values[m].name] = run.values[m].value\n","\n","  metrics.update(run.assignments)\n","\n","  df_bs_20 = df_bs_20.append(metrics, ignore_index=True)\n","\n","df_bs_20['peak_memory'] = df_bs_20['peak_memory'] / 10000.0\n","\n","hue_bs_20 = pd.DataFrame([(df_bs_20['accuracy'] > constraints['accuracy']),\n","                          (df_bs_20['inference_time'] < constraints['inference_time']),\n","                          (df_bs_20['peak_memory'] < constraints['peak_memory']),\n","                          (df_bs_20['storage_memory'] < constraints['storage_memory'])]).all()\n","\n","hue_bs_20.name = 'Constraints'\n","\n","hue_bs_20[hue_bs_20 == False] = 0\n","hue_bs_20[hue_bs_20 == True] = 1\n","\n","df_bs_20 = pd.concat([df_bs_20, hue_bs_20], axis = 1)"],"metadata":{"id":"EugJFbZaxN6Y"},"id":"EugJFbZaxN6Y","execution_count":null,"outputs":[]},{"cell_type":"code","source":["range_accuracy = [min(df_bs_10['accuracy'].min(),\n","                      df_bs_20['accuracy'].min()),\n","                  max(df_bs_10['accuracy'].max(),\n","                      df_bs_20['accuracy'].max())]\n","\n","range_peak_memory = [min(df_bs_10['peak_memory'].min(),\n","                         df_bs_20['peak_memory'].min()),\n","                     max(df_bs_10['peak_memory'].max(),\n","                         df_bs_20['peak_memory'].max())]\n","\n","range_storage_memory = [min(df_bs_10['storage_memory'].min(),\n","                            df_bs_20['storage_memory'].min()),\n","                        max(df_bs_10['storage_memory'].max(),\n","                            df_bs_20['storage_memory'].max())]\n","\n","range_inference_time = [min(df_bs_10['inference_time'].min(),\n","                            df_bs_20['inference_time'].min()),\n","                        max(df_bs_10['inference_time'].max(),\n","                            df_bs_20['inference_time'].max())]"],"metadata":{"id":"maVck4dGxX1y"},"id":"maVck4dGxX1y","execution_count":null,"outputs":[]},{"cell_type":"code","source":["dimensions_hyperparameters = list([\n","                                   dict(range = [8, 64], label = 'batchsize', values = df_bs_10['batchsize']),\n","                                   dict(range = [3, 9], label = 'conv1_kernel_size', values = df_bs_10['conv1_kernel_size']),\n","                                   dict(range = [16, 64], label = 'conv1_out_channels', values = df_bs_10['conv1_out_channels']),\n","                                   dict(range = [3, 9], label = 'conv2_kernel_size', values = df_bs_10['conv2_kernel_size']),\n","                                   dict(range = [32, 128], label = 'conv2_out_channels', values = df_bs_10['conv2_out_channels']),\n","                                   dict(range = [3, 9], label = 'conv3_kernel_size', values = df_bs_10['conv3_kernel_size']),\n","                                   dict(range = [64, 256], label = 'conv3_out_channels', values = df_bs_10['conv3_out_channels']),\n","                                   dict(range = [0.01, 0.5], label = 'dropout_probability', values = df_bs_10['dropout_probability']),\n","                                   dict(range = [3, 20], label = 'epochs', values = df_bs_10['epochs']),\n","                                   dict(range = [256, 1024], label = 'fc1_out_features', values = df_bs_10['fc1_out_features']),\n","                                   dict(range = [64, 512]\t, label = 'fc2_out_features', values = df_bs_10['fc2_out_features']),\n","                                   dict(range = [0.00001, 0.5], label = 'learning_rate', values = df_bs_10['learning_rate']),\n","                                   dict(range = [2, 3], label = 'pool_kernel_size', values = df_bs_10['pool_kernel_size']),\n","                                   ])\n","\n","dimensions_metrics = list([\n","                           dict(range = range_accuracy, label = 'accuracy', values = df_bs_10['accuracy']),\n","                           dict(range = range_peak_memory, label = 'peak_memory', values = df_bs_10['peak_memory']),\n","                           dict(range = range_storage_memory, label = 'storage_memory', values = df_bs_10['storage_memory']),\n","                           dict(range = range_inference_time, label = 'inference_time', values = df_bs_10['inference_time']),\n","                           ])\n","\n","line_experiment = dict(color = df_bs_10['Constraints'], colorscale = [[0,'#1f77b4'], [1,'#ff7f0e']])\n","\n","fig = go.Figure(data=go.Parcoords(line = line_experiment,\n","                                  dimensions = dimensions_metrics+dimensions_hyperparameters\n","                                  ))\n","\n","fig.update_layout(\n","    plot_bgcolor = 'white',\n","    paper_bgcolor = 'white',\n","    title_text='Test Batch Size 10',\n","    title_x=0.5)\n","\n","fig.show()"],"metadata":{"id":"nzwvfPa9xhgN"},"id":"nzwvfPa9xhgN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["dimensions_hyperparameters = list([\n","                                   dict(range = [8, 64], label = 'batchsize', values = df_bs_10['batchsize']),\n","                                   dict(range = [3, 9], label = 'conv1_kernel_size', values = df_bs_10['conv1_kernel_size']),\n","                                   dict(range = [16, 64], label = 'conv1_out_channels', values = df_bs_10['conv1_out_channels']),\n","                                   dict(range = [3, 9], label = 'conv2_kernel_size', values = df_bs_10['conv2_kernel_size']),\n","                                   dict(range = [32, 128], label = 'conv2_out_channels', values = df_bs_10['conv2_out_channels']),\n","                                   dict(range = [3, 9], label = 'conv3_kernel_size', values = df_bs_10['conv3_kernel_size']),\n","                                   dict(range = [64, 256], label = 'conv3_out_channels', values = df_bs_10['conv3_out_channels']),\n","                                   dict(range = [0.01, 0.5], label = 'dropout_probability', values = df_bs_10['dropout_probability']),\n","                                   dict(range = [3, 20], label = 'epochs', values = df_bs_10['epochs']),\n","                                   dict(range = [256, 1024], label = 'fc1_out_features', values = df_bs_10['fc1_out_features']),\n","                                   dict(range = [64, 512]\t, label = 'fc2_out_features', values = df_bs_10['fc2_out_features']),\n","                                   dict(range = [0.00001, 0.5], label = 'learning_rate', values = df_bs_10['learning_rate']),\n","                                   dict(range = [2, 3], label = 'pool_kernel_size', values = df_bs_10['pool_kernel_size']),\n","                                   ])\n","\n","dimensions_metrics = list([\n","                           dict(range = range_accuracy, label = 'accuracy', values = df_bs_10['accuracy']),\n","                           dict(range = range_peak_memory, label = 'peak_memory', values = df_bs_10['peak_memory']),\n","                           dict(range = range_storage_memory, label = 'storage_memory', values = df_bs_10['storage_memory']),\n","                           dict(range = range_inference_time, label = 'inference_time', values = df_bs_10['inference_time']),\n","                           ])\n","\n","line_experiment = dict(color = df_bs_10['Constraints'], colorscale = [[0,'#1f77b4'], [1,'#ff7f0e']])\n","\n","fig = go.Figure(data=go.Parcoords(line = line_experiment,\n","                                  dimensions = dimensions_metrics#+dimensions_hyperparameters,\n","                                  ))\n","\n","fig.update_layout(\n","    plot_bgcolor = 'white',\n","    paper_bgcolor = 'white',\n","    title_text='Test Batch Size 10',\n","    title_x=0.5,\n","    width=500,\n","    height=500,)\n","\n","fig.show()"],"metadata":{"id":"xSNA4clWxmwl"},"id":"xSNA4clWxmwl","execution_count":null,"outputs":[]},{"cell_type":"code","source":["dimensions_hyperparameters = list([\n","                                   dict(range = [8, 64], label = 'batchsize', values = df_bs_20['batchsize']),\n","                                   dict(range = [3, 9], label = 'conv1_kernel_size', values = df_bs_20['conv1_kernel_size']),\n","                                   dict(range = [16, 64], label = 'conv1_out_channels', values = df_bs_20['conv1_out_channels']),\n","                                   dict(range = [3, 9], label = 'conv2_kernel_size', values = df_bs_20['conv2_kernel_size']),\n","                                   dict(range = [32, 128], label = 'conv2_out_channels', values = df_bs_20['conv2_out_channels']),\n","                                   dict(range = [3, 9], label = 'conv3_kernel_size', values = df_bs_20['conv3_kernel_size']),\n","                                   dict(range = [64, 256], label = 'conv3_out_channels', values = df_bs_20['conv3_out_channels']),\n","                                   dict(range = [0.01, 0.5], label = 'dropout_probability', values = df_bs_20['dropout_probability']),\n","                                   dict(range = [3, 20], label = 'epochs', values = df_bs_20['epochs']),\n","                                   dict(range = [256, 1024], label = 'fc1_out_features', values = df_bs_20['fc1_out_features']),\n","                                   dict(range = [64, 512]\t, label = 'fc2_out_features', values = df_bs_20['fc2_out_features']),\n","                                   dict(range = [0.00001, 0.5], label = 'learning_rate', values = df_bs_20['learning_rate']),\n","                                   dict(range = [2, 3], label = 'pool_kernel_size', values = df_bs_20['pool_kernel_size']),\n","                                   ])\n","\n","dimensions_metrics = list([\n","                           dict(range = range_accuracy, label = 'accuracy', values = df_bs_20['accuracy']),\n","                           dict(range = range_peak_memory, label = 'peak_memory', values = df_bs_20['peak_memory']),\n","                           dict(range = range_storage_memory, label = 'storage_memory', values = df_bs_20['storage_memory']),\n","                           dict(range = range_inference_time, label = 'inference_time', values = df_bs_20['inference_time']),\n","                           ])\n","\n","line_experiment = dict(color = df_bs_20['Constraints'], colorscale = [[0,'#1f77b4'], [1,'#ff7f0e']])\n","\n","fig = go.Figure(data=go.Parcoords(line = line_experiment,\n","                                  dimensions = dimensions_metrics+dimensions_hyperparameters\n","                                  ))\n","\n","fig.update_layout(\n","    plot_bgcolor = 'white',\n","    paper_bgcolor = 'white',\n","    title_text='Test Batch Size 20',\n","    title_x=0.5)\n","\n","fig.show()"],"metadata":{"id":"PTGXBZtaxuXS"},"id":"PTGXBZtaxuXS","execution_count":null,"outputs":[]},{"cell_type":"code","source":["dimensions_hyperparameters = list([\n","                                   dict(range = [8, 64], label = 'batchsize', values = df_bs_20['batchsize']),\n","                                   dict(range = [3, 9], label = 'conv1_kernel_size', values = df_bs_20['conv1_kernel_size']),\n","                                   dict(range = [16, 64], label = 'conv1_out_channels', values = df_bs_20['conv1_out_channels']),\n","                                   dict(range = [3, 9], label = 'conv2_kernel_size', values = df_bs_20['conv2_kernel_size']),\n","                                   dict(range = [32, 128], label = 'conv2_out_channels', values = df_bs_20['conv2_out_channels']),\n","                                   dict(range = [3, 9], label = 'conv3_kernel_size', values = df_bs_20['conv3_kernel_size']),\n","                                   dict(range = [64, 256], label = 'conv3_out_channels', values = df_bs_20['conv3_out_channels']),\n","                                   dict(range = [0.01, 0.5], label = 'dropout_probability', values = df_bs_20['dropout_probability']),\n","                                   dict(range = [3, 20], label = 'epochs', values = df_bs_20['epochs']),\n","                                   dict(range = [256, 1024], label = 'fc1_out_features', values = df_bs_20['fc1_out_features']),\n","                                   dict(range = [64, 512]\t, label = 'fc2_out_features', values = df_bs_20['fc2_out_features']),\n","                                   dict(range = [0.00001, 0.5], label = 'learning_rate', values = df_bs_20['learning_rate']),\n","                                   dict(range = [2, 3], label = 'pool_kernel_size', values = df_bs_20['pool_kernel_size']),\n","                                   ])\n","\n","dimensions_metrics = list([\n","                           dict(range = range_accuracy, label = 'accuracy', values = df_bs_20['accuracy']),\n","                           dict(range = range_peak_memory, label = 'peak_memory', values = df_bs_20['peak_memory']),\n","                           dict(range = range_storage_memory, label = 'storage_memory', values = df_bs_20['storage_memory']),\n","                           dict(range = range_inference_time, label = 'inference_time', values = df_bs_20['inference_time']),\n","                           ])\n","\n","line_experiment = dict(color = df_bs_20['Constraints'], colorscale = [[0,'#1f77b4'], [1,'#ff7f0e']])\n","\n","fig = go.Figure(data=go.Parcoords(line = line_experiment,\n","                                  dimensions = dimensions_metrics#+dimensions_hyperparameters\n","                                  ))\n","\n","fig.update_layout(\n","    plot_bgcolor = 'white',\n","    paper_bgcolor = 'white',\n","    title_text='Test Batch Size 20',\n","    title_x=0.5,\n","    width=500,\n","    height=500,)\n","\n","fig.show()"],"metadata":{"id":"RdSaK-_rx29A"},"id":"RdSaK-_rx29A","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"colab":{"name":"AutoML_2022_Efficient Development of Application Specific Machine Learning Models-Copy2.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}